{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessities for the analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline  \n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.ensemble import BalancedBaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df_churn=pd.read_csv('data/Churn_prepared.csv',index_col='customerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1)\n",
    "pruner = MedianPruner(n_startup_trials=10)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scaler= StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your training data for h1n1\n",
    "X = df_churn.drop(columns=['Churn'])\n",
    "y = df_churn['Churn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def objective_logistic(trial):\n",
    "    # Define the hyperparameters to search\n",
    "    params = {\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l2']),\n",
    "        'C': trial.suggest_float('C', 1e-4, 1e4, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'sag', 'newton-cg']),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 2000),\n",
    "        'tol': trial.suggest_float('tol', 1e-6, 1e-2),\n",
    "        'fit_intercept': True,\n",
    "    }\n",
    "\n",
    "    # Create a pipeline with scaling and logistic regression\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('model', LogisticRegression(**params))  \n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation and calculate the average AUC score\n",
    "    auc_scores = cross_val_score(pipe, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean AUC score across all folds\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study_logistic = optuna.create_study(direction='maximize',sampler=sampler,pruner=pruner)\n",
    "study_logistic.optimize(objective_logistic, n_trials=250)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_logistic = study_logistic.best_params\n",
    "\n",
    "model_logreg = LogisticRegression(**best_params_logistic,random_state=1)\n",
    "joblib.dump(model_logreg, 'logistic_regression_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for Logistic Regression: {best_params_logistic}\")\n",
    "print(\"Model saved as 'logistic_regression_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the objective function for tuning Random Forest\n",
    "def objective_random_forest(trial):\n",
    "    # Define the hyperparameters to search\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),  # Number of trees\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 60),  # Max tree depth\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),  # Min samples to split\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),  # Min samples at a leaf\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),  # Max features to split\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),  # Bootstrap sampling\n",
    "    }\n",
    "\n",
    "    # Create a Random Forest model with the hyperparameters\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "\n",
    "    # Perform cross-validation and calculate the average accuracy score\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean accuracy score across all folds\n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Create and run the Optuna study for Random Forest\n",
    "study_rf = optuna.create_study(direction='maximize',sampler=sampler,pruner=pruner)\n",
    "study_rf.optimize(objective_random_forest, n_trials=200)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_rf = study_rf.best_params\n",
    "\n",
    "# Save the model (with best hyperparameters, not fitted) using joblib\n",
    "model_rf = RandomForestClassifier(**best_params_rf,random_state=1)\n",
    "joblib.dump(model_rf, 'random_forest_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for Random Forest: {best_params_rf}\")\n",
    "print(\"Model saved as 'random_forest_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the objective function for tuning SVC with scaling\n",
    "def objective_svc(trial):\n",
    "    # Define the hyperparameters to search\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])  # Kernel\n",
    "    \n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-3, 1e3, log=True),  # Regularization \n",
    "        'degree': trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3,  # Degree for 'poly' kernel\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),  # Kernel coefficient\n",
    "        'tol': trial.suggest_float('tol', 1e-6, 1e-2),  # Tolerance for stopping\n",
    "    }\n",
    "\n",
    "    # Create a pipeline with scaling and SVC\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling\n",
    "        ('svc', SVC(kernel=kernel, **params, probability=True, random_state=1))  \n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation and calculate the average ROC AUC score\n",
    "    accuracy_scores = cross_val_score(pipe, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean ROC AUC score across all folds\n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Create and run the Optuna study for SVC\n",
    "study_svc = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n",
    "study_svc.optimize(objective_svc, n_trials=200)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_svc = study_svc.best_params\n",
    "\n",
    "# Save the model (with best hyperparameters, not fitted) using joblib\n",
    "model_svc = SVC(kernel=best_params_svc['kernel'], **{k: v for k, v in best_params_svc.items() if k != 'kernel'}, probability=True, random_state=1)\n",
    "joblib.dump(model_svc, 'svc_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for SVC: {best_params_svc}\")\n",
    "print(\"Model saved as 'svc_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Gradient Boosting Classifier Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the objective function for tuning XGBoost\n",
    "def objective_xgb(trial):\n",
    "    # Define the hyperparameters to search\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),  # Number of boosting rounds\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),  # Max depth of trees\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),  # Learning rate (eta)\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),  # Subsampling ratio\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),  # Feature subsampling\n",
    "        'objective': 'binary:logistic',  # Binary classification\n",
    "        'eval_metric': 'logloss',  # Evaluation metric\n",
    "        'use_label_encoder': False  # Avoid unnecessary warning\n",
    "    }\n",
    "\n",
    "    # Create an XGBoost model with the hyperparameters\n",
    "    model = xgb.XGBClassifier(**params, random_state=1)\n",
    "\n",
    "    # Perform cross-validation and calculate the average accuracy score\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean accuracy score across all folds\n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Create and run the Optuna study for XGBoost\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n",
    "study_xgb.optimize(objective_xgb, n_trials=300)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_xgb = study_xgb.best_params\n",
    "\n",
    "# Save the model (with best hyperparameters, not fitted) using joblib\n",
    "model_xgb = xgb.XGBClassifier(**best_params_xgb,random_state=1)\n",
    "joblib.dump(model_xgb, 'xgb_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for XGBoost: {best_params_xgb}\")\n",
    "print(\"Model saved as 'xgb_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Preceptron Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlp(trial):\n",
    "    # Define the hyperparameters to search\n",
    "    params = {\n",
    "        'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(10,), (20,), (30,), (50,), (100,),(50,25),(100,50)]),  # Number of neurons in each layer\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),  # Activation function\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),  # Optimization algorithm\n",
    "        'alpha': trial.suggest_float('alpha', 1e-5, 1e-1, log=True),  # L2 penalty (regularization term)\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),  # Learning rate schedule\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000),  # Maximum number of iterations\n",
    "    }\n",
    "\n",
    "    # Create a pipeline with scaling and MLP\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scaling\n",
    "        ('mlp', MLPClassifier(**params, random_state=1))  # MLP model\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation and calculate the average accuracy score\n",
    "    accuracy_scores = cross_val_score(pipe, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean accuracy score across all folds\n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Create and run the Optuna study for MLP\n",
    "study_mlp = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n",
    "study_mlp.optimize(objective_mlp, n_trials=100, n_jobs=-1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_mlp = study_mlp.best_params\n",
    "\n",
    "# Save the model (with best hyperparameters, not fitted) using joblib\n",
    "model_mlp = MLPClassifier(**best_params_mlp, random_state=1)\n",
    "joblib.dump(model_mlp, 'mlp_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for MLP: {best_params_mlp}\")\n",
    "print(\"Model saved as 'mlp_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for tuning Gaussian Naive Bayes\n",
    "def objective_naive_bayes(trial):\n",
    "    # Naive Bayes typically has fewer hyperparameters, but we can tune 'var_smoothing'\n",
    "    params = {\n",
    "        'var_smoothing': trial.suggest_float('var_smoothing', 1e-9, 1e-1, log=True)  # Smoothing parameter\n",
    "    }\n",
    "\n",
    "    # Create a Gaussian Naive Bayes model with the hyperparameters\n",
    "    model = GaussianNB(**params)\n",
    "\n",
    "    # Perform cross-validation and calculate the average accuracy score\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Return the mean accuracy score across all folds\n",
    "    return np.mean(accuracy_scores)\n",
    "\n",
    "# Create and run the Optuna study for Naive Bayes\n",
    "study_nb = optuna.create_study(direction='maximize', sampler=sampler, pruner=pruner)\n",
    "study_nb.optimize(objective_naive_bayes, n_trials=200)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_nb = study_nb.best_params\n",
    "model_gnb = GaussianNB(**best_params_nb)\n",
    "joblib.dump(model_gnb, 'naive_bayes_model.pkl')\n",
    "\n",
    "print(f\"Best hyperparameters for Naive Bayes: {best_params_nb}\")\n",
    "print(\"Model saved as 'naive_bayes_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
